# Cloud-Based-Data-Pipeline-Development
Overview

This project demonstrates an end-to-end ETL (Extract, Transform, Load) pipeline built using Python, SQL, and PySpark on AWS. The pipeline processes a sample dataset of 5 million rows, performs transformations to enable business analytics, and stores the results in AWS S3 as a structured data lake. The project showcases my skills in data engineering, cloud technologies, and big data tools.

Key Features





Data Processing: Processed 5 million rows of sample data using Python and PySpark.



ETL Workflow: Extracted data, performed transformations (e.g., aggregations), and loaded results into AWS S3.



Performance Optimization: Achieved a 30% reduction in processing time through optimized PySpark workflows.



SQL Querying: Improved query performance by 20% for reporting purposes using SQL-based analysis.



Data Lake: Implemented a structured data lake architecture in AWS S3 for efficient storage and retrieval.

Tech Stack





Languages: Python, SQL



Big Data Tools: PySpark, Apache Spark



Cloud Platform: AWS (S3)



Libraries: pandas, boto3
